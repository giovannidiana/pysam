{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian inference with pySam\n",
    "pySam provides a simple implementation of Monte Carlo techniques such as importance sampling and sequential MC for static models.\n",
    "\n",
    "The package contains a base class for Bayesian models `pysam.Model`\n",
    "\n",
    "```python\n",
    "class Model:\n",
    "    \n",
    "    def __init__(self): raise NotImplementedError\n",
    "    def LogLikelihood(self,X): raise NotImplementedError\n",
    "    def LogPrior(self,X): raise NotImplementedError\n",
    "    def rPrior(self,X): raise NotImplementedError\n",
    "    def Density(self,X): raise NotImplementedError\n",
    "```\n",
    "where `LogLikelihood`, `LogPrior` and `Density` are the data log likelihood, the log of the prior distribution of the model parameters, and `Density` is the probabilistic model. The user implementation of these functions must support vectorization, i.e. `X` can be a numpy array with set of parameters arranged by rows.\n",
    "\n",
    "The main class implementing sampling methods is `pysam.Inference`. An instance of `pysam.Inference` is specified by a Model object\n",
    "\n",
    "```python\n",
    "def __init__(self,model):\n",
    "        self.model = model\n",
    "```\n",
    "\n",
    "## Importance sampling \n",
    "Posterior averages can be obtained by using weighted samples obtained from an importance distribution used to approximate the posterior density. Importance weights are obtained as the ratio between the unnormalized posterior and the importance densities.\n",
    "\n",
    "## Sequential Monte Carlo \n",
    "In pySam we implemented a tempered likelihood scheme to obtain a particle approximation of the posterior density. The method `pysam.SMC` corresponds to the following algorithm[1]\n",
    "\n",
    "1. Initialization \n",
    "     * set $n=0$ \n",
    "     * draw $N$ particles corresponding to independent model parameters $\\theta_{1:N}$ according to their prior distributions\n",
    "     * Set importance weights to $1/N$ for all particles\n",
    "     * Define a tempered protocol $h=\\lbrace h_0=0,h_1,\\cdots,h_{P-1},h_P=1\\rbrace$\n",
    "     * Iterate steps 2, 3 and 4\n",
    "2. Resampling\n",
    "     * Calculate the effective sample size (ESS)\n",
    "     $$ESS = \\left(\\sum_k (W^{(k)})^2\\right)^{-1}$$\n",
    "     * when $ESS<N/2$ resample all particles and set their weights to $1/N$\n",
    "3. Reweighting \n",
    "     * Reweight all particles using an incremental log weight of \n",
    "      $\\log\\tilde w = (h_{n+1}-h_{n}) \\log P(data|\\theta)$\n",
    "     * recalculate and normalize all weights \n",
    "4. Move particles according to a Metropolis-Hastings kernel. \n",
    "\n",
    "## References\n",
    "[1] Del Moral, Pierre, Arnaud Doucet, and Ajay Jasra. \"Sequential monte carlo samplers.\" Journal of the Royal Statistical Society: Series B (Statistical Methodology) 68.3 (2006): 411-436."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
